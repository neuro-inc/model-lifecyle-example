{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f231f8d",
   "metadata": {},
   "source": [
    "## Preparations\n",
    "\n",
    "Make you you have started a MLFlow application from Apolo Console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d13f719",
   "metadata": {},
   "source": [
    "## Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda2ec6b-0131-4852-88d6-4aab698b1641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn numpy mlflow skl2onnx onnxruntime apolo-sdk apolo-extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f1ecf-4d3d-4e28-a526-a9b6834a2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.client\n",
    "import mlflow.experiments\n",
    "import mlflow\n",
    "import mlflow.models\n",
    "import mlflow.tracking\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import argparse\n",
    "from sklearn.datasets import load_iris\n",
    "import skl2onnx\n",
    "import os\n",
    "import apolo_sdk\n",
    "import asyncio\n",
    "\n",
    "from mlflow import (\n",
    "    get_run,\n",
    "    start_run,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e224362c",
   "metadata": {},
   "source": [
    "This function uses Apolo SDK to get the current MLFlow instance URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b23dc6-81fd-4eea-8510-d9fa352f193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_mlflow_url():\n",
    "    async with apolo_sdk.get() as client:\n",
    "        jobs = []\n",
    "        async with client.jobs.list(tags=[\"job:mlflow\"], statuses=[apolo_sdk.JobStatus.RUNNING]) as job_iter:\n",
    "            jobs += [job async for job in job_iter]\n",
    "        async with client.jobs.list(tags=[\"platform-app-type:mlflow\"], statuses=[apolo_sdk.JobStatus.RUNNING]) as job_iter:\n",
    "            jobs += [job async for job in job_iter]\n",
    "        if len(jobs) > 0:\n",
    "            print(\"MLFlow instance found. Logging model metadata\")\n",
    "            return \"http://\"+str(jobs[0].internal_hostname)\n",
    "        print(\"MLFlow instance not found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7ddb1-14e8-4223-9af6-6d93dc7a3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_uri = os.environ.get(\"MLFLOW_TRACKING_URI\", await get_mlflow_url())\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "print(tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f62be",
   "metadata": {},
   "source": [
    "Start training and logging the model, its parameters and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc5c83b-66e8-45d6-b703-3fafac2afe7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with start_run() as run:\n",
    "    print(\"Logging params\")\n",
    "    mlflow.log_param(\"param1\", \"value1\")\n",
    "    mlflow.log_metric(\"metric1\", 1.0)\n",
    "    print(\"Setting tag\")\n",
    "    mlflow.set_tag(\"tag1\", \"value1\")\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    fetched_run = get_run(run_id)\n",
    "\n",
    "    print(\"Saving artifact\")\n",
    "    content = \"artifact content\"\n",
    "    with open(\"test_artifact.txt\", \"w\") as f:\n",
    "        f.write(content)\n",
    "    mlflow.log_artifact(\"test_artifact.txt\")\n",
    "    \n",
    "    print(\"Getting artifact\")\n",
    "    run_id = run.info.run_id\n",
    "    artifact_uri = mlflow.get_artifact_uri(\"test_artifact.txt\")\n",
    "    \n",
    "    print(\"Traning model\")\n",
    "    data = load_iris()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "    sc = StandardScaler()\n",
    "\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "\n",
    "    model = Perceptron(max_iter=40, eta0=0.1, random_state=0)\n",
    "    model.fit(X_train_std, y_train)\n",
    "    y_pred = model.predict(X_test_std)\n",
    "\n",
    "    print('Misclassfied samples: %d' % (y_test != y_pred).sum())\n",
    "    \n",
    "    signature = mlflow.models.infer_signature(X_train_std, model.predict(X_train_std))\n",
    "    mlflow.sklearn.log_model(model, \"model\", signature=signature)\n",
    "    model_uri = f\"runs:/{run.info.run_id}/model\"\n",
    "    print(\"Registering model\")\n",
    "    registered_model = mlflow.register_model(model_uri, \"iris_perceptron\")\n",
    "    \n",
    "    assert registered_model.name == \"iris_perceptron\"\n",
    "    # assert registered_model.version == '1'\n",
    "\n",
    "    loaded_model = mlflow.sklearn.load_model(f\"models:/{registered_model.name}/1\")\n",
    "    prediction = loaded_model.predict([[0.1, 0.5, 0.8, 0.2]])\n",
    "\n",
    "    # assert isinstance(loaded_model, Perceptron)\n",
    "    print(loaded_model)\n",
    "    assert prediction.shape == (1,)\n",
    "    \n",
    "    assert fetched_run.data.params[\"param1\"] == \"value1\"\n",
    "    assert fetched_run.data.metrics[\"metric1\"] == 1.0\n",
    "\n",
    "    options = {id(model): {\"zipmap\": False}}\n",
    "    onx = skl2onnx.to_onnx(model, X_train_std.astype(np.float32), options=options)\n",
    "    mlflow.onnx.log_model(onx, \"onnx_model\", signature=signature)\n",
    "\n",
    "    print(\"Registering ONNX model\")\n",
    "    onnx_model_uri = f\"runs:/{run.info.run_id}/onnx_model\"\n",
    "    registered_model = mlflow.register_model(onnx_model_uri, \"onnx_iris_perceptron\")\n",
    "\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    print(\"Registered models: \", client.search_registered_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218bd42c",
   "metadata": {},
   "source": [
    "## Starting an Inference Server\n",
    "\n",
    "Go to Apolo Console, start a Apolo Deploy application, select your model from the list and start an inference server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991bfeef",
   "metadata": {},
   "source": [
    "After you've successfully started an inference server with Apolo Deploy, open the Jupyter Notebook called `inference-demo.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
